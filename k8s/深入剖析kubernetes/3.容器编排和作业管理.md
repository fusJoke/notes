## 为什么需要pod

1.容器的本质就是进程
2.容器镜像就是系统中的.exe安装包
3.Kubernetes是操作系统
4.进程不是单独存在而是以进程组方式组织在一起的。

pod就是进程组的概念在容器技术的映射。



### pod解决了什么问题

```
容器没有能力管理多个进程的能力，容器yo里 PID=1 的进程就是应用本身，其他的进程都是这个 PID=1 进程的子进程。
```

Pod 是 Kubernetes 里的原子调度单位。 k8s项目的调度器，是统一按照 Pod 而非容器的资源需求进行计算的

“超亲密关系”容器的特征：有直接的文件交换、使用 localhost 或者 Socket 文件进行本地通信、会发生非常频繁的远程调用、需要共享某些 Linux Namespace。



#### **容器设计模式**

pod只是一个逻辑上的概念，Kubernetes 处理的还是宿主机操作系统上 Linux 容器的 Namespace 和 Cgroups。

Pod是一组共享了某些资源的容器组，共享的是同一个 Network Namespace，并且可以声明共享同一个 Volume。

Pod 中，永远先创建Infra 容器，而其他用户定义的容器，则通过 Join Network Namespace 的方式，与 Infra 容器关联在一起。Infra 容器占用极少的资源，它使用的是一个非常特殊的镜像，叫作：`k8s.gcr.io/pause`

Pod 里的容器 A 和容器 B 来说：

- 它们可以直接使用 localhost 进行通信；
- 它们看到的网络设备跟 Infra 容器看到的完全一样；
- 一个 Pod 只有一个 IP 地址，也就是这个 Pod 的 Network Namespace 对应的 IP 地址；
- 当然，其他的所有网络资源，都是一个 Pod 一份，并且被该 Pod 中的所有容器共享；
- Pod 的生命周期只跟 Infra 容器一致，而与容器 A 和 B 无关。

一个 Volume 对应的宿主机目录对于 Pod 来说就只有一个，Pod 里的容器只要声明挂载这个 Volume，就一定可以共享这个 Volume 对应的宿主机目录。

在 Pod 中，所有 Init Container 定义的容器，都会比 spec.containers 定义的用户容器先启动。并且，Init Container 容器会按顺序逐一启动，而直到它们都启动并且退出了，用户容器才会启动。

启动顺序：Init Container 定义的容器 > spec.containers 定义的用户容器 > 用户容器才会启动

容器设计模式里最常用的一种模式，它的名字叫：sidecar，可以在一个 Pod 中，启动一个辅助容器，来完成一些独立于主进程（主容器）之外的工作	



**调度、网络、存储，以及安全相关的属性，基本上是 Pod 级别的**



**NodeSelector：是一个供用户将 Pod 与 Node 进行绑定的字段**

```
apiVersion: v1
kind: Pod
...
spec:
 nodeSelector:
   disktype: ssd
```

**NodeName**：这个字段一般由调度器负责设置，但用户也可以设置它来“骗过”调度器

**HostAliases：定义了 Pod 的 hosts 文件（比如 /etc/hosts）里的内容**

```
...
spec:
  hostAliases:
  - ip: "10.1.2.3"
    hostnames:
    - "foo.remote"
    - "bar.remote"
...
```

**shareProcessNamespace**：Pod 里的容器要共享 PID Namespace

**ImagePullPolicy**: 默认是 Always，即每次创建 Pod 都重新拉取一次镜像

**Lifecycle**: 容器状态发生变化时触发一系列“钩子”



##### Pod生命周期的生命周期

1. Pending。Pod 的 YAML 文件已经提交给了 Kubernetes，API 对象已经被创建并保存在 Etcd 当中。但是，这个 Pod 里有些容器因为某种原因而不能被顺利创建。
2. Running。这个状态下，Pod 已经调度成功，跟一个具体的节点绑定。它包含的容器都已经创建成功，并且至少有一个正在运行中
3. Succeeded。这个状态意味着，Pod 里的所有容器都正常运行完毕，并且已经退出了。这种情况在运行一次性任务时最为常见
4. Failed。这个状态下，Pod 里至少有一个容器以不正常的状态（非 0 的返回码）退出。这个状态的出现
5. Unknown。这是一个异常状态，意味着 Pod 的状态不能持续地被 kubelet 汇报给 kube-apiserver，这很有可能是主从节点（Master 和 Kubelet）间的通信出现了问题



## 特殊的volume

作用：为容器提供预先定义好的数据。

Volume 里的信息就是仿佛是**被 Kubernetes“投射”（Project）进入容器当中的**。这正是 Projected Volume 

+ secret
  把 Pod 想要访问的加密数据，存放到 Etcd 中。然后通过在 Pod 的容器里挂载 Volume 的方式，访问到这些 Secret 里保存的信息
  
  ```
  ...
  metadata:
    name: test-projected-volume 
  spec:
    containers:
    - name: test-secret-volume
      image: busybox
      args:
      - sleep
      - "86400"
      volumeMounts:
      - name: mysql-cred
        mountPath: "/projected-volume"
        readOnly: true
    volumes:
    - name: mysql-cred
      projected:
        sources:
        - secret:
            name: user
        - secret:
            name: pass
  ```
+ configMap
  ConfigMap 保存的是不需要加密的、应用所需的配置信息, 用法和secret几乎一样。kubectl create configmap 从文件或者目录创建 ConfigMap，也可以直接编写 ConfigMap 对象的 YAML 文件
+ downward api
  让 Pod 里的容器能够直接获取到这个 Pod API 对象本身的信息
+ serviceAccountToken



## Service Account

Service Account 的授权信息和文件，实际上保存在它所绑定的一个特殊的 Secret 对象里的**ServiceAccountToken**

**Kubernetes 客户端以容器的方式运行在集群里，然后使用 default Service Account 自动授权的方式，被称作“InClusterConfig”，也是我最推荐的进行 Kubernetes API 编程的授权方式**

容器健康检查和恢复机制

在 Kubernetes 中，你可以为 Pod 里的容器定义一个健康检查“探针”（Probe）。这样，kubelet 就会根据这个 Probe 的返回值决定这个容器的状态，而不是直接以容器进行是否运行

Kubernetes 中并没有 Docker 的 Stop 语义。所以虽然是 Restart（重启），但实际却是重新创建了容器



容器的健康检查和恢复机制

**只要 Pod 的 restartPolicy 指定的策略允许重启异常的容器（比如：Always），那么这个 Pod 就会保持 Running 状态，并进行容器重启**

**对于包含多个容器的 Pod，只有它里面所有的容器都进入异常状态后，Pod 才会进入 Failed 状态**





## 控制器模型

Kubernetes 操作这些“集装箱”的逻辑，都由控制器（Controller）完成。

kube-controller-manager 的组件是一系列控制器的集合。控制器被统一放在 pkg/controller 的原因是它们都要遵循控制循环（control loop）

用一种对象管理另一种对象，这个控制器对象本身，负责定义被管理对象的期望状态。

被控制对象的定义，则来自于一个“模板” -- PodTemplate。

```
for {
  实际状态 := 获取集群中对象 X 的实际状态（Actual State）
  期望状态 := 获取集群中对象 X 的期望状态（Desired State）
  if 实际状态 == 期望状态{
    什么都不做
  } else {
    执行编排动作，将实际状态调整为期望状态
  }
}
//实际状态: kubelet 通过心跳汇报的容器状态和节点状态，或者监控系统中保存的应用监控数据，或者控制器主动收集的它自己感兴趣的信息
//期望状态: 期望状态，一般来自于用户提交的 YAML 文件
```

<img src="/Users/wangfusheng/Documents/notes/k8s/深入剖析kubernetes/.assets/image-20220812102534744-0271136.png" alt="image-20220812102534744" style="zoom:50%;" /> 





## 作业副本和水平扩展

Deployment 的 Pod 模板被更新，那么 Deployment 就以“滚动更新”（rolling update）的方式来升级现有的容器——ReplicaSet

**一个 ReplicaSet 对象，其实就是由副本数目的定义和一个 Pod 模板组成的**

 <img src="/Users/wangfusheng/Documents/notes/k8s/深入剖析kubernetes/.assets/image-20220812110730648-0273652.png" alt="image-20220812110730648" style="zoom:50%;" />

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3  #Pod 副本个数是 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
```

**一个ReplicaSet对象就是由副本数目的定义和一个Pod模板组成**；

**Deployment控制器实际操纵是ReplicaSet，而不是Pod对象**。

```
kubectl apply -f nginx-deployment.yaml
#水平扩展/收缩
kubectl scale deployment nginx-deployment --replicas=4
```



```shell
$ kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   4/4     4            4           4m28s

DESIRED：用户期望的 Pod 副本个数（spec.replicas 的值）；
CURRENT：当前处于 Running 状态的 Pod 的个数；
UP-TO-DATE：当前处于最新版本的 Pod 的个数，所谓最新版本指的是 Pod 的 Spec 部分与 Deployment 里 Pod 模板里定义的完全一致；
AVAILABLE：当前已经可用的 Pod 的个数，即：既是 Running 状态，又是最新版本，并且已经处于 Ready（健康检查正确）状态的 Pod 的个数。
```

```
#查看deployment实时变化
kubectl rollout status deployment/nginx-deployment
```

```
#kubectl edit把 API 对象的内容下载到了本地文件，让你修改完成后再提交上去
kubectl edit deployment/nginx-deployment
```

```
kubectl rollout status deployment/nginx-deployment
kubectl describe deployment nginx-deployment
```

<img src="/Users/wangfusheng/Documents/notes/k8s/深入剖析kubernetes/.assets/image-20220813090517456-0352719.png" alt="image-20220813090517456" style="zoom: 55%;" /> 





### StatefulSet

k8s对有状态应用初步支持，statefulset。

1.拓扑状态。应用的多个实例之间不是完全对等的关系

2.存储状态。应用的多个实例分别绑定了不同的存储数据

**StatefulSet 的核心功能，就是通过某种方式记录这些状态，然后在 Pod 被重新创建时，能够为新 Pod 恢复这些状态**

一个 Kubernetes 项目中非常实用的概念：Headless Service。
service将一组pod暴露给外界访问的一种机制。比如，一个 Deployment 有 3 个 Pod，那么我就可以定义一个 Service。然后，用户只要能访问到这个 Service，它就能访问到某个具体的 Pod。

service如何被访问？

**第一种方式，是以 Service 的 VIP（Virtual IP，即：虚拟 IP）方式**. 比如：当我访问 10.0.23.1 这个 Service 的 IP 地址时，10.0.23.1 其实就是一个 VIP，它会把请求转发到该 Service 所代理的某一个 Pod 上

**第二种方式，就是以 Service 的 DNS 方式**.

​		Normal Service: 你访问“my-svc.my-namespace.svc.cluster.local”解析到的，正是 my-svc 这个 Service 的 VIP

​		Headless Service: 这种情况下，你访问“my-svc.my-namespace.svc.cluster.local”解析到的，直接就是 my-svc 代理的某一个 Pod 的 IP 地址

```
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None  
  selector:
    app: nginx
```

```
kubectl create -f svc.yaml
kubectl get service nginx
kubectl create -f statefulset.yaml
kubectl get statefulset web
```

```
$ kubectl run -i --tty --image busybox:1.28.4 dns-test --restart=Never --rm /bin/sh
If you don't see a command prompt, try pressing enter.
/ # nslookup web-0.nginx
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.nginx
Address 1: 172.17.0.10 web-0.nginx.default.svc.cluster.local
/ # nslookup web-1.nginx
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-1.nginx
Address 1: 172.17.0.12 web-1.nginx.default.svc.cluster.local
/ #
```

**StatefulSet 就保证了 Pod 网络标识的稳定性**

```
StatefulSet 这个控制器的主要作用之一，就是使用 Pod 模板创建 Pod 的时候，对它们进行编号，并且按照编号顺序逐一完成创建工作。而当 StatefulSet 的“控制循环”发现 Pod 的“实际状态”与“期望状态”不一致，需要新建或者删除 Pod 进行“调谐”的时候，它会严格按照这些 Pod 编号的顺序，逐一完成这些操作
```

通过 Headless Service 的方式，StatefulSet 为每个 Pod 创建了一个固定并且稳定的 DNS 记录，来作为它的访问入口

在部署“有状态应用”的时候，应用的每个实例拥有唯一并且稳定的“网络标识”，是一个非常重要的假设



#### PVC和PV

Kubernetes 项目引入了一组叫作 Persistent Volume Claim（PVC）和 Persistent Volume（PV）的 API 对象，大大降低了用户声明和使用持久化 Volume 的门槛

定义一个pvc，声明想要的volume的属性

```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pv-claim
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
```

在应用的pod中，声明使用这个pvc

```
apiVersion: v1
kind: Pod
metadata:
  name: pv-pod
spec:
  containers:
    - name: pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: pv-storage
  volumes:
    - name: pv-storage
      persistentVolumeClaim:
        claimName: pv-claim
```

只要我们创建这个 PVC 对象，Kubernetes 就会自动为它绑定一个符合条件的 Volume。符合条件的 Volume 来自于由运维人员维护的 PV（Persistent Volume）对象

```
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-volume
  labels:
    type: local
spec:
  capacity:
    storage: 10Gi
  rbd:
    monitors:
    - '10.16.154.78:6789'
    - '10.16.154.82:6789'
    - '10.16.154.83:6789'
    pool: kube
    image: foo
    fsType: ext4
    readOnly: true
    user: admin
    keyring: /etc/ceph/keyring
    imageformat: "2"
    imagefeatures: "layering"
```

Kubernetes 中 PVC 和 PV 的设计，**实际上类似于“接口”和“实现”的思想**

```
$ kubectl get pvc -l app=nginx
NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
www-web-0   Bound    pvc-f785717e-93cd-4842-a6c4-8dba9fc6ca0e   1Gi        RWO            standard       9h
www-web-1   Bound    pvc-a7e95c9f-6ab8-4f7d-86cb-0220ee512210   1Gi        RWO            standard       9h
```

 StatefulSet 创建出来的所有 Pod，都会声明使用编号的 PVC

```
for i in 0 1; do kubectl exec web-$i -- sh -c 'echo hello $(hostname) > /usr/share/nginx/html/index.html'; done
```

StatefulSet 其实是一种特殊的 Deployment，它的每个 Pod 都被编号了。这个编号会体现在 Pod 的名字和 hostname 等标识信息上，这不仅代表了 Pod 的创建顺序，也是 Pod 的重要网络标识

有了这个编号后，StatefulSet 就使用 Kubernetes 里的两个标准功能：Headless Service 和 PV/PVC，实现了对 Pod 的拓扑状态和存储状态的维护



## 配置mysql主从

三要点：

1. Master 节点和 Slave 节点需要有不同的配置文件（即：不同的 my.cnf）；
2. Master 节点和 Salve 节点需要能够传输备份信息文件；
3. 在 Slave 节点第一次启动之前，需要执行一些初始化 SQL 操作；

configMap

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql
  labels:
    app: mysql
data:
  master.cnf: |
    # 主节点 MySQL 的配置文件
    [mysqld]
    log-bin
  slave.cnf: |
    # 从节点 MySQL 的配置文件
    [mysqld]
    super-read-only
```

Service

```
apiVersion: v1
kind: Service
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  ports:
  - name: mysql
    port: 3306
  clusterIP: None
  selector:
    app: mysql
---
apiVersion: v1
kind: Service
metadata:
  name: mysql-read
  labels:
    app: mysql
spec:
  ports:
  - name: mysql
    port: 3306
  selector:
    app: mysql
```



### 容器化守护经进程的意义

1. 这个 Pod 运行在 Kubernetes 集群里的每一个节点（Node）上；
2. 每个节点上只有一个这样的 Pod 实例；
3. 当有新的节点加入 Kubernetes 集群后，该 Pod 会自动地在新节点上被创建出来；而当旧节点被删除后，它上面的 Pod 也相应地会被回收掉。

**DaemonSet 其实是一个非常简单的控制器**。在它的控制循环中，只需要遍历所有节点，然后根据节点上是否有被管理 Pod 的情况，来决定是否要创建或者删除一个 Pod。

```
apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: metadata.name
            operator: In
            values:
            - node-geektime
```

DaemonSet 还会给 Pod 自动加上 tolerations 字段。

Toleration：所有被标记为 unschedulable“污点”的 Node，“容忍”的效果是允许调度。

正常情况下，被标记了 unschedulable“污点”的 Node，是不会有任何 Pod 被调度上去的。







## 离线业务job和cronjob

job api

```
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
      - name: pi
        image: resouer/ubuntu-bc 
        command: ["sh", "-c", "echo 'scale=10000; 4*a(1)' | bc -l "]
      restartPolicy: Never
  backoffLimit: 4
```



## 声明式api与k8s编程范式

kubectl apply 命令

kubectl replace 的执行过程，是使用新的 YAML 文件中的 API 对象，**替换原有的 API 对象**；而 kubectl apply，则是执行了一个**对原有 API 对象的 PATCH 操作**

这意味着 kube-apiserver 在响应命令式请求（比如，kubectl replace）的时候，一次只能处理一个写请求，否则会有产生冲突的可能。而对于声明式请求（比如，kubectl apply），**一次能处理多个写操作，并且具备 Merge 能力**





### 声明式api

在 Kubernetes 项目中，一个 API 对象在 Etcd 里的完整资源路径，是由：Group（API 组）、Version（API 版本）和 Resource（API 资源类型）三个部分组成的。

<img src="/Users/wangfusheng/Documents/notes/k8s/深入剖析kubernetes/.assets/image-20220818152610603-0807571.png" alt="image-20220818152610603" style="zoom:50%;" /> 



<img src="/Users/wangfusheng/Documents/notes/k8s/深入剖析kubernetes/.assets/image-20220818161456196-0810497.png" alt="image-20220818161456196" style="zoom: 50%;" /> 



