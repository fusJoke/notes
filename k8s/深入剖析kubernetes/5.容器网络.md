网络栈：网卡（Network Interface）、回环设备（Loopback Device）、路由表（Routing Table）和 iptables 规则。

在linux能起到虚拟交换机作用的网络设备就是网桥，

<img src="/Users/wangfusheng/Documents/notes/k8s/深入剖析kubernetes/.assets/image-20220815141907156.png" alt="image-20220815141907156" style="zoom:50%;" /> 

**被限制在 Network Namespace 里的容器进程，实际上是通过 Veth Pair 设备 + 宿主机网桥的方式，实现了跟同其他容器的数据交换**

<img src="/Users/wangfusheng/Documents/notes/k8s/深入剖析kubernetes/.assets/image-20220824114409342-1312650.png" alt="image-20220824114409342" style="zoom:50%;" />  

我们需要在已有的宿主机网络上，再通过软件构建一个覆盖在已有宿主机网络之上的、可以把所有容器连通在一起的虚拟网络。所以，这种技术就被称为：Overlay Network（覆盖网络）





## 容器跨宿主机通信

不同宿主机上的容器通过ip地址进行访问是做不到的。

Flannel 项目是 CoreOS 公司主推的容器网络方案：

1. VXLAN；
2. host-gw；
3. UDP。

udp模式跨主网络

- 宿主机 Node 1 上有一个容器 container-1，它的 IP 地址是 100.96.1.2，对应的 docker0 网桥的地址是：100.96.1.1/24。
- 宿主机 Node 2 上有一个容器 container-2，它的 IP 地址是 100.96.2.3，对应的 docker0 网桥的地址是：100.96.2.1/24。

我们现在的任务，就是让 container-1 访问 container-2。

container-1 容器里的进程发起的 IP 包，其源地址就是 100.96.1.2，目的地址就是 100.96.2.3。由于目的地址 100.96.2.3 并不在 Node 1 的 docker0 网桥的网段里，所以这个 IP 包会被交给默认路由规则，通过容器的网关进入 docker0 网桥（如果是同一台宿主机上的容器间通信，走的是直连规则），从而出现在宿主机上。

在 Linux 中，TUN 设备是一种工作在三层（Network Layer）的虚拟网络设备。TUN 设备的功能非常简单，即：**在操作系统内核和用户应用程序之间传递 IP 包。**

**flanneld 又是如何知道这个 IP 地址对应的容器，是运行在 Node 2 上的呢？**

 

<img src="/Users/wangfusheng/Documents/notes/k8s/深入剖析kubernetes/.assets/image-20220903075954794-2163195.png" alt="image-20220903075954794" style="zoom:67%;" /> 

<img src="/Users/wangfusheng/Documents/notes/k8s/深入剖析kubernetes/.assets/image-20220903080246172-2163367.png" alt="image-20220903080246172" style="zoom:67%;" /> 

第一次：用户态的容器进程发出的 IP 包经过 docker0 网桥进入内核态；

第二次：IP 包根据路由表进入 TUN（flannel0）设备，从而回到用户态的 flanneld 进程；

第三次：flanneld 进行 UDP 封包之后重新进入内核态，将 UDP 包通过宿主机的 eth0 发出去。



VXLAN 模式，即 Virtual Extensible LAN（虚拟可扩展局域网），是 Linux 内核本身就支持的一种网络虚似化技术。

VXLAN 的覆盖网络的设计思想是：在现有的三层网络之上，“覆盖”一层虚拟的、由内核 VXLAN 模块负责维护的二层网络，使得连接在这个 VXLAN 二层网络上的“主机”（虚拟机或者容器都可以）之间，可以像在同一个局域网（LAN）里那样自由通信。当然，实际上，这些“主机”可能分布在不同的宿主机上，甚至是分布在不同的物理机房里。

而为了能够在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧道”的两端。这个设备就叫作 VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点）。

而 VTEP 设备的作用，其实跟前面的 flanneld 进程非常相似。只不过，它进行封装和解封装的对象，是二层数据帧（Ethernet frame）；而且这个工作的执行流程，全部是在内核里完成的（因为 VXLAN 本身就是 Linux 内核中的一个模块）。

<img src="/Users/wangfusheng/Documents/notes/k8s/深入剖析kubernetes/.assets/image-20220903081230140-2163951.png" alt="image-20220903081230140" style="zoom:67%;" /> 

![image-20220904132023305](/Users/wangfusheng/Documents/notes/k8s/深入剖析kubernetes/.assets/image-20220904132023305.png)

# Kubernetes网络模型与CNI网络插件



![image-20220903094415278](/Users/wangfusheng/Documents/notes/k8s/深入剖析kubernetes/.assets/image-20220903094415278-2169456.png) 

Kubernetes 之所以要设置这样一个与 docker0 网桥功能几乎一样的 CNI 网桥，主要原因包括两个方面：

- Kubernetes 没有使用 Docker 的网络模型（CNM），所以不具备配置 docker0 网桥的能力；
- 与 Infra 容器的 Network Namespace 密切相关。

cni设计思想：**Kubernetes 在启动 Infra 容器之后，就可以直接调用 CNI 网络插件，为这个 Infra 容器的 Network Namespace，配置符合预期的网络栈**。



CNI 的基础可执行文件，按照功能可以分为三类：

**第一类，叫作 Main 插件，它是用来创建具体网络设备的二进制文件**

**第二类，叫作 IPAM（IP Address Management）插件，它是负责分配 IP 地址的二进制文件**

**第三类，是由 CNI 社区维护的内置 CNI 插件**

在 Kubernetes 中，处理容器网络相关的逻辑并不会在 kubelet 主干代码里执行，而是会在具体的 CRI（Container Runtime Interface，容器运行时接口）实现里完成。对于 Docker 项目来说，它的 CRI 实现叫作 dockershim，你可以在 kubelet 的代码里找到它







# Kubernetes三层网络方案

<img src="/Users/wangfusheng/Documents/notes/k8s/深入剖析kubernetes/.assets/image-20220904133440117-2269682.png" alt="image-20220904133440117" style="zoom:50%;" /> 

<img src="/Users/wangfusheng/Documents/notes/k8s/深入剖析kubernetes/.assets/image-20220904143127775.png" alt="image-20220904143127775" style="zoom:67%;" /> 

Calico 的 CNI 插件会为每个容器设置一个 Veth Pair 设备，然后把其中的一端放置在宿主机上



<img src="/Users/wangfusheng/Documents/notes/k8s/深入剖析kubernetes/.assets/image-20220904144709476-2274030.png" alt="image-20220904144709476" style="zoom:67%;" /> 

# 为什么说Kubernetes只有soft multi-tenancy？

网络隔离能力的定义，NetworkPolicy。

一旦 Pod 被 NetworkPolicy 选中，**那么这个 Pod 就会进入“拒绝所有”（Deny All）的状态**，即：这个 Pod 既不允许被外界访问，也不允许对外界发起访问。

**而 NetworkPolicy 定义的规则，其实就是“白名单”。**

1. 该隔离规则只对 default Namespace 下的，携带了 role=db 标签的 Pod 有效。限制的请求类型包括 ingress（流入）和 egress（流出）。
2. Kubernetes 会拒绝任何访问被隔离 Pod 的请求，除非这个请求来自于以下“白名单”里的对象，并且访问的是被隔离 Pod 的 6379 端口。这些“白名单”对象包括：
3. default Namespace 里的，携带了 role=fronted 标签的 Pod；
4. 任何 Namespace 里的、携带了 project=myproject 标签的 Pod；
5. 任何源地址属于 172.17.0.0/16 网段，且不属于 172.17.1.0/24 网段的请求。
6. Kubernetes 会拒绝被隔离 Pod 对外发起任何请求，除非请求的目的地址属于 10.0.0.0/24 网段，并且访问的是该网段地址的 5978 端口。

**Kubernetes 网络插件对 Pod 进行隔离，其实是靠在宿主机上生成 NetworkPolicy 对应的 iptable 规则来实现的**





# Service、DNS与服务发现
