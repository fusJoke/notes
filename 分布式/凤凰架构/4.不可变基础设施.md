# 不可变基础设施

## 虚拟化容器

一个计算机软件要能够正确运行，需要三个方面的兼容性共同保障：

+ ISA兼容：目标机器指令兼容性
+ ABI兼容：目标系统或者依赖库的二进制兼容
+ 环境兼容：目标环境的兼容性，譬如没有正确设置的配置文件、环境变量、注册中心、数据库地址、文件系统的权限等

```
指令集架构（Instruction Set Architecture，ISA）是计算机体系结构中与程序设计有关的部分，包含了基本数据类型，指令集，寄存器，寻址模式，存储体系，中断，异常处理以及外部 I/O。指令集架构包含一系列的 Opcode 操作码（即通常所说的机器语言），以及由特定处理器执行的基本命令
应用二进制接口（Application Binary Interface，ABI）是应用程序与操作系统之间或其他依赖库之间的低级接口。ABI 涵盖了各种底层细节，如数据类型的宽度大小、对象的布局、接口调用约定等等。ABI 不同于应用程序接口（Application Programming Interface，API），API 定义的是源代码和库之间的接口，因此同样的代码可以在支持这个 API 的任何系统中编译，而 ABI 允许编译好的目标代码在使用兼容 ABI 的系统中无需改动就能直接运行
```

虚拟化技术：

+ 指令虚拟化，通过软件来模拟不同 ISA 架构的处理器工作过程，将虚拟机发出的指令转换为符合本机 ISA 的指令
+ 硬件抽象层虚拟化，以软件或者直接通过硬件来模拟处理器、芯片组、内存、磁盘控制器、显卡等设备的工作过程。既可以使用纯软件的二进制翻译来模拟虚拟设备，也可以由硬件的Intel VT-d、AMD-Vi这类虚拟化技术，将某个物理设备直通到虚拟机中使用，代表为VMware ESXi和Hyper-V。
+ 操作系统层面的虚拟化(容器化)，操作系统层虚拟化则不会提供真实的操作系统，而是采用隔离手段，使得不同进程拥有独立的系统资源和资源配额，看起来仿佛是独享了整个操作系统一般，其实系统的内核仍然是被不同进程所共享的。
+ 运行库虚拟化，操作系统虚拟化采用隔离手段来模拟系统不同，运行库虚拟化选择使用软件翻译的方法来模拟系统，它以一个独立进程来代替操作系统内核来提供目标软件运行所需的全部能力
+ **语言层虚拟化**虚拟机将高级语言生成的中间代码转换为目标机器可以直接执行的指令

### 容器的崛起

#### 隔离文件： chroot

`chroot`操作会锁定进程的根目录在参数所指定的位置，之后进程及进程的子进程将不能再访问和操作该目录之外文件。而`pivot_root` 直接切换了根文件系统（rootfs），有效避免了chroot的漏洞。

#### 隔离访问：namespaces

Linux 的namespaces是一种由内核直接提供的全局资源封装，是内核针对进程设计的访问隔离机制。进程在一个独立的 Linux 名称空间中朝系统看去，拥有这台 Linux 主机上的一切资源，文件系统，PID 编号（拥有0号进程--系统初始化的进程）、UID/GID 编号（譬如拥有自己独立的 root 用户）、网络（譬如完全独立的 IP 地址、网络栈、防火墙等设置）

| 名称空间 | 隔离内容                                                     | 内核版本 |
| :------- | ------------------------------------------------------------ | -------- |
| Mount    | 隔离文件系统，功能上大致可以类比`chroot`                     | 2.4.19   |
| UTS      | 隔离主机的[Hostname](https://en.wikipedia.org/wiki/Hostname)、[Domain names](https://en.wikipedia.org/wiki/Domain_name) | 2.6.19   |
| IPC      | 隔离进程间通信的渠道（详见“[远程服务调用](https://icyfenix.cn/architect-perspective/general-architecture/api-style/rpc.html)”中对 IPC 的介绍） | 2.6.19   |
| PID      | 隔离进程编号，无法看到其他名称空间中的 PID，意味着无法对其他进程产生影响 | 2.6.24   |
| Network  | 隔离网络资源，如网卡、网络栈、IP 地址、端口，等等            | 2.6.29   |
| User     | 隔离用户和用户组                                             | 3.8      |
| Cgroup   | 隔离`cgroups`信息，进程有自己的`cgroups`的根目录视图（在/proc/self/cgroup 不会看到整个系统的信息）。`cgroups`的话题很重要，稍后笔者会安排一整节来介绍 | 4.6      |
| Time     | 隔离系统时间，2020 年 3 月最新的 5.6 内核开始支持进程独立设置系统时间 | 5.6      |

#### 隔离资源：cgroups

cgroups用于隔离或者说分配并限制某个进程组能够使用的资源配额，资源配额包括处理器时间、内存大小、磁盘 I/O 速度，等等

| 控制组子系统 | 功能                                                         |
| ------------ | ------------------------------------------------------------ |
| blkio        | 为块设备（如磁盘，固态硬盘，USB 等等）设定 I/O 限额。        |
| cpu          | 控制`cgroups`中进程的处理器占用比率。                        |
| cpuacct      | 自动生成`cgroups`中进程所使用的处理器时间的报告。            |
| cpuset       | 为`cgroups`中的进程分配独立的处理器（包括多路系统的处理器，多核系统的处理器核心）。 |
| devices      | 设置`cgroups`中的进程访问某个设备的权限（读、写、创建三种权限）。 |
| freezer      | 挂起或者恢复`cgroups`中的进程。                              |
| memory       | 设定`cgroups`中进程使用内存的限制，并自动生成内存资源使用报告。 |
| net_cls      | 使用等级识别符标记网络数据包，可允许 Linux 流量控制程序识别从具体 `cgroups`中生成的数据包。 |
| net_prio     | 用来设置网络流量的优先级。                                   |
| hugetlb      | 主要针对于 HugeTLB 系统进行限制。                            |
| perf_event   | 允许 Perf 工具基于`cgroups`分组做性能监测。                  |

#### 封装系统：LXC

LXC 眼中的容器的定义是一种封装系统的轻量级虚拟机。仍是按照先装系统然再装软件的思路。

#### 封装应用：Docker

docker的容器化能力来源于LXC，镜像分层组合的文件系统来源于aufs。

+ **跨机器的绿色部署**：Docker 定义了一种将应用及其所有的环境依赖都打包到一起的格式。
+ **以应用为中心的封装**：Docker 封装应用而非封装机器的理念贯穿了它的设计、API、界面、文档等多个方面
+ **自动构建**：Docker 提供了开发人员从在容器中构建产品的全部支持，开发人员无需关注目标机器的具体配置，即可使用任意的构建工具链，在容器中自动构建出最终产品
+ **多版本支持**：Docker 支持像 Git 一样管理容器的连续版本，进行检查版本间差异、提交或者回滚等操作
+ **组件重用**：Docker 允许将任何现有容器作为基础镜像来使用，以此构建出更加专业的镜像。
+ **共享**：Docker 拥有公共的镜像仓库， Docker 用户在上面上传了自己的镜像，同时也使用他人上传的镜像。
+ **工具生态**：Docker 开放了一套可自动化和自行扩展的接口

运行时标准定义了应该如何运行一个容器、如何管理容器的状态和生命周期、如何使用操作系统的底层特性`namespaces`、`cgroup`、`pivot_root`等；

容器镜像标准规定了容器镜像的格式、配置、元数据的格式，可以理解为对镜像的静态描述；镜像分发标准则规定了镜像推送和拉取的网络交互过程

#### 封装集群：kubernetes

Kubernetes 为代表的容器编排框架，就是把大型软件系统运行所依赖的集群环境也进行了虚拟化，令集群得以实现跨数据中心的绿色部署，并能够根据实际情况自动扩缩

<img src="/Users/wangfusheng/Documents/notes/分布式/凤凰架构/.assets/image-20220824220441400-1349882.png" alt="image-20220824220441400" style="zoom: 33%;" /> 

2016 年，Kubernetes 1.5 版本开始引入“[容器运行时接口](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/)”（Container Runtime Interface，CRI），这是一个定义容器运行时应该如何接入到 kubelet 的规范标准，从此 Kubernetes 内部的 DockerManager 就被更为通用的 KubeGenericRuntimeManager 所替代

```
Kubernetes Master → kubelet → KubeGenericRuntimeManager → containerd → runC
```



### 以容器构建系统

一个容器封装一个单进程应用已经成为被广泛认可的最佳实践。

应用需要多个进程共同协作，通过集群的形式对外提供服务，以虚拟化方法实现这个目标的过程就被称为容器编排（Container Orchestration）

容器之间顺畅地交互通信是协作的核心需求，但容器协作并不仅仅是将容器以高速网络互相连接而已。如何调度容器，如何分配资源，如何扩缩规模，如何最大限度地接管系统中的非功能特性，让业务系统尽可能免受分布式复杂性的困扰都是容器编排框架必须考虑的问题

### 隔离与协作

设计一套容器编排系统

###### 假设你现在有两个应用，其中一个是 Nginx，另一个是为该 Nginx 收集日志的 Filebeat，你希望将它们封装为容器镜像，以方便日后分发。

最直接的方案就将 Nginx 和 Filebeat 直接编译成同一个容器镜像。但它违背了单个容器封装单进程应用的最佳实践。Dockerfile 只允许有一个 ENTRYPOINT，因为 Docker 只能通过监视 PID 为 1 的进程（即由 ENTRYPOINT 启动的进程）的运行状态来判断容器的工作状态是否正常，容器退出执行清理，容器崩溃自动重启等操作都必须先判断状态。

###### 假设有两个 Docker 镜像，一个 Nginx 容器，一个 Filebeat 容器。现在要求 Filebeat 容器能收集 Nginx 容器产生的日志信息。

 Nginx 容器和 Filebeat 容器启动时，将日志目录和收集目录挂载为宿主机同一个磁盘位置的 Volume 即可。同一进程组中的多个进程可共享着相同的访问权限与资源配额。把容器与进程在概念上对应起来，那容器编排的第一个扩展点，就是要找到容器领域中与“进程组”相对应的概念，这是实现容器从隔离到协作的第一步，在 Kubernetes 的设计里，这个对应物叫作 Pod。

超亲密的协作是特指多个容器位于同一个 Pod 这种特殊关系，它们将默认共享

- **UTS 名称空间**：所有容器都有相同的主机名和域名。
- **网络名称空间**：所有容器都共享一样的网卡、网络栈、IP 地址
- **IPC 名称空间**：所有容器都可以通过信号量或者 POSIX 共享内存等方式通信
- **时间名称空间**：所有容器都共享相同的系统时间

Pod 的另外一个基本职责是实现原子性调度，因为要跨节点调度。

###### 假设你现在有 Filebeat、Nginx 两个 Docker 镜像，在一个具有多个节点的集群环境下，要求每次调度都必须让 Filebeat 和 Nginx 容器运行于同一个节点上。

协同调度（Coscheduling）的概念，以保证一组紧密联系的任务能够被同时分配资源。Pod 是隔离与调度的基本单位。Kubernetes 将一切皆视为资源，不同资源之间依靠层级关系相互组合协作

<img src="/Users/wangfusheng/Documents/notes/分布式/凤凰架构/.assets/image-20220830234856604-1874538.png" alt="image-20220830234856604" style="zoom:50%;" /> 



- **容器**（Container）：一个容器封装一个应用进程，是镜像管理的最小单位。
- **生产任务**（Pod）：进程组对应的“容器组”的概念，Pod 中容器共享 UTS、IPC、网络，是资源调度的最小单位。
- **节点**（Node）：对应于集群中的单台机器，即可是生产环境中的物理机，也可以是云计算环境中的虚拟节点，节点是处理器和内存等资源的资源池，是硬件单元的最小单位。
- **集群**（Cluster）：对应于整个集群，Kubernetes 提倡理念是面向集群来管理应用。当你要部署应用的时候，只需要通过声明式 API 将你的意图写成一份元数据（Manifests），将它提交给集群即可，而无需关心它具体分配到哪个节点、如何实现 Pod 间通信、如何保证韧性与弹性，等等，所以集群是处理元数据的最小单位。
- **集群联邦**（Federation）：对应于多个集群，通过联邦可以统一管理多个 Kubernetes 集群，联邦的一种常见应用是支持跨可用区域多活、跨地域容灾的需求。

### 韧性与弹性

###### **场景四**：假设有一个由数十个 Node、数百个 Pod、近千个 Container 所组成的分布式系统，要避免系统因为外部流量压力、代码缺陷、软件更新、硬件升级、资源分配等各种原因而出现中断，作为管理员，你希望编排系统能为你提供何种支持？

编排系统在服务出现问题，运行状态不正确的时候，能自动将它们调整成正确的状态。应对的解决办法在工业控制系统里已经有非常成熟的应用，叫作控制回路（Control Loop）

`spec`字段所描述的便是资源的期望状态

```
额外知识：Kubernates 的资源对象与控制器

目前，Kubernetes 已内置支持相当多的资源对象，并且还可以使用CRD（Custom Resource Definition）来自定义扩充，你可以使用kubectl api-resources来查看它们。笔者根据用途分类列举了以下常见的资源：

用于描述如何创建、销毁、更新、扩缩 Pod，包括：Autoscaling（HPA）、CronJob、DaemonSet、Deployment、Job、Pod、ReplicaSet、StatefulSet
用于配置信息的设置与更新，包括：ConfigMap、Secret
用于持久性地存储文件或者 Pod 之间的文件共享，包括：Volume、LocalVolume、PersistentVolume、PersistentVolumeClaim、StorageClass
用于维护网络通信和服务访问的安全，包括：SecurityContext、ServiceAccount、Endpoint、NetworkPolicy
用于定义服务与访问，包括：Ingress、Service、EndpointSlice
用于划分虚拟集群、节点和资源配额，包括：Namespace、Node、ResourceQuota
这些资源在控制器管理框架中一般都会有相应的控制器来管理，笔者列举常见的控制器，按照它们的启动情况分类如下：

必须启用的控制器：EndpointController、ReplicationController、PodGCController、ResourceQuotaController、NamespaceController、ServiceAccountController、GarbageCollectorController、DaemonSetController、JobController、DeploymentController、ReplicaSetController、HPAController、DisruptionController、StatefulSetController、CronJobController、CSRSigningController、CSRApprovingController、TTLController
默认启用的可选控制器，可通过选项禁止：TokenController、NodeController、ServiceController、RouteController、PVBinderController、AttachDetachController
默认禁止的可选控制器，可通过选项启用：BootstrapSignerController、TokenCleanerController
```

实际状态有可能发生变化的资源对象都会由对应的控制器进行追踪，每个控制器至少会追踪一种类型的资源。

控制器管理框架（[kube-controller-manager](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-controller-manager/)）来维护这些资源控制器的正常运作

指标监视器（[kube-apiserver](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-apiserver/)）来为控制器工作时提供其追踪资源的度量数据。

**场景五**：通过服务编排，对任何分布式系统自动实现以下三种通用的能力：

1. Pod 出现故障时，能够自动恢复，不中断服务；
2. Pod 更新程序时，能够滚动更新，不中断服务；
3. Pod 遇到压力时，能够水平扩展，不中断服务；

Pod 本身也是资源，正确的做法是通过副本集（ReplicaSet）来创建 Pod。ReplicaSet 也是一种资源，是属于工作负荷一类的资源，它代表一个或多个 Pod 副本的集合（即`spec.replicas`的值）。当 ReplicaSet 成功创建之后，副本集控制器就会持续跟踪该资源，如果一旦有 Pod 发生崩溃退出，或者状态异常，ReplicaSet 都会自动创建新的 Pod 来替代异常的 Pod；如果异常多出现了额外数量的 Pod，也会被 ReplicaSet 自动回收掉，总之就是确保任何时候集群中这个 Pod 副本的数量都向期望状态靠拢

滚动更新（Rolling Updates）是指先停止少量旧副本，维持大量旧副本继续提供服务，当停止的旧副本更新成功，新副本可以提供服务以后，再重复以上操作，直至所有的副本都更新成功。将这个过程放到 ReplicaSet 上，就是先创建新版本的 ReplicaSet，然后一边让新 ReplicaSet 逐步创建新版 Pod 的副本，一边让旧的 ReplicaSet 逐渐减少旧版 Pod 的副本。

如果你希望改变某个资源的某种状态，应该将期望状态告诉 Kubernetes，而不是去教 Kubernetes 具体该如何操作。因此，新的部署资源（Deployment）与部署控制器被设计出来，可以由 Deployment 来创建 ReplicaSet，再由 ReplicaSet 来创建 Pod，当你更新 Deployment 中的信息（譬如更新了镜像的版本）以后，部署控制器就会跟踪到你新的期望状态，自动地创建新 ReplicaSet，并逐渐缩减旧的 ReplicaSet 的副本数，直至升级完成后彻底删除掉旧 ReplicaSet

<img src="/Users/wangfusheng/Documents/notes/分布式/凤凰架构/.assets/image-20220901182849163-2028130.png" alt="image-20220901182849163" style="zoom:67%;" /> 



对于场景五的最后一种情况，遇到流量压力时，管理员完全可以手动修改 Deployment 中的副本数量，或者通过`kubectl scale`命令指定副本数量，促使 Kubernetes 部署更多的 Pod 副本来应对压力，然而这种扩容方式不仅需要人工参与，且只靠人类经验来判断需要扩容的副本数量，不容易做到精确与及时。为此 Kubernetes 又提供了 Autoscaling 资源和自动扩缩控制器，能够自动根据度量指标，如处理器、内存占用率、用户自定义的度量值等，来设置 Deployment（或者 ReplicaSet）的期望状态，实现当度量指标出现变化时，系统自动按照“Autoscaling→Deployment→ReplicaSet→Pod”这样的顺序层层变更，最终实现根据度量指标自动扩容缩容。



### 以应用为中心的封装

#### Kustomize

Kubernetes 官方给出“如何封装应用”的解决方案是“用配置文件来配置配置文件”。Kustomize 的主要价值是根据环境来生成不同的部署配置

#### Helm 与 Chart

如果说 Kubernetes 是云原生操作系统的话，那 Helm 就要成为这个操作系统上面的应用商店与包管理工具

#### Operator 与 CRD

有状态应用的多个应用实例之间往往有着特定的拓扑关系与顺序关系。

Kubernetes 从 1.9 版本开始正式发布了 StatefulSet 及对应的 StatefulSetController。

由 StatefulSet 管理的 Pod 具备以下几项额外特性：

- **Pod 会按顺序创建和按顺序销毁**：StatefulSet 中的各个 Pod 会按顺序地创建出来，创建后续的 Pod 前，必须要保证前面的 Pod 已经转入就绪状态。删除 StatefulSet 中的 Pod 时会按照与创建顺序的逆序来执行。
- **Pod 具有稳定的网络名称**：Kubernetes 中的 Pod 都具有唯一的名称，在普通的副本集中这是靠随机字符产生的，而在 StatefulSet 中管理的 Pod，会以带有顺序的编号作为名称，且能够在重启后依然保持不变。。
- **Pod 具有稳定的持久存储**：StatefulSet 中的每个 Pod 都可以拥有自己独立的 PersistentVolumeClaim 资源。即使 Pod 被重新调度到其它节点上，它所拥有的持久磁盘也依然会被挂载到该 Pod，这点会在“容器持久化”中进一步介绍。

由 Operator 开发者自己编码实现的控制器完全可以在原地对 Pod 进行重启，而无需像 Deployment 那样必须先删除旧 Pod，然后再创建新 Pod。

使用 CRD 定义高层次资源、使用配套的控制器来维护期望状态，带来的好处不仅仅是“高级指令”的便捷，而是遵循 Kubernetes 一贯基于资源与控制器的设计原则的同时，又不必再受制于 Kubernetes 内置资源的表达能力。只要 Operator 的开发者愿意编写代码，前面曾经提到那些 StatfulSet 不能支持的能力，如备份恢复数据、创建删除索引、调整平衡策略等操作，都完全可以实现出来。

#### 开放应用模型

开放应用模型把云原生应用定义为“由一组相互关联但又离散独立的组件构成，这些组件实例化在合适的运行时上，由配置来控制行为并共同协作提供统一的功能”。

```
OAM 定义的应用
一个Application由一组Components构成，每个Component的运行状态由Workload描述，每个Component可以施加Traits来获取额外的运维能力，同时我们可以使用Application Scopes将Components划分到一或者多个应用边界中，便于统一做配置、限制、管理。把Components、Traits和Scopes组合在一起实例化部署，形成具体的Application Configuration，以便解决应用的多实例部署与升级。
```

- **服务组件**（Components）：由 Component 构成应用的思想自 SOA 以来就屡见不鲜，然而 OAM 的 Component 不仅仅是特指构成应用“整体”的一个“部分”，它还有一个重要职责是抽象那些应该由开发人员去关注的元素。譬如应用的名字、自述、容器镜像、运行所需的参数，等等。
- **工作负荷**（Workload）：Workload 决定了应用的运行模式，每个 Component 都要设定自己的 Workload 类型，OAM 按照“是否可访问、是否可复制、是否长期运行”预定义了六种 Workload 类型，如表 11-2 所示。如有必要还可以通过 CRD 与 Operator 去扩展。
- **运维特征**（Traits）：开发活动有大量复用功能的技巧，但运维活动却很贫乏，平时能为运维写个 Shell 脚本或者简单工具已经算是个高级的运维人员了。OAM 的 Traits 就用于封装模块化后的运维能力，可以针对运维中的可重复操作预先设定好一些具体的 Traits，譬如日志收集 Trait、负载均衡 Trait、水平扩缩容 Trait，等等。 这些预定义的 Traits 定义里，会注明它们可以作用于哪种类型的工作负荷、包含能填哪些参数、哪些必填选填项、参数的作用描述是什么，等等。
- **应用边界**（Application Scopes）：多个 Component 共同组成一个 Scope，你可以根据 Component 的特性或者作用域来划分 Scope，譬如具有相同网络策略的 Component 放在同一个 Scope 中，具有相同健康度量策略的 Component 放到另一个 Scope 中。同时，一个 Component 也可能属于多个 Scope，譬如一个 Component 完全可能既需要配置网络策略，也需要配置健康度量策略。
- **应用配置**（Application Configuration）：将 Component（必须）、Trait（必须）、Scope（非必须）组合到一起进行实例化，就形成了一个完整的应用配置。



## 容器的网络

### linux的网络虚拟化

虚拟化容器是以 Linux 名称空间的隔离性为基础来实现的，那解决隔离的容器之间、容器与宿主机之间、乃至跨物理网络的不同容器间通信问题的责任，很自然也落在了 Linux 网络虚拟化技术。

#### 网络通信模型

Linux 系统的通信过程无论按理论上的 OSI 七层模型，还是以实际上的 TCP/IP 四层模型来解构，都明显地呈现出“逐层调用，逐层封装”的特点，这种逐层处理的方式与栈结构，譬如程序执行时的方法栈很类似，因此它通常被称为“[Linux 网络协议栈](https://en.wikipedia.org/wiki/Protocol_stack)”，简称“网络栈”，有时也称“协议栈”

<img src="/Users/wangfusheng/Documents/notes/分布式/凤凰架构/.assets/image-20220825113447433-1398488.png" alt="image-20220825113447433" style="zoom:50%;" /> 

+ socket：应用层的程序是通过 Socket 编程接口来和内核空间的网络协议栈通信的。
+ tcp/udp：tcp，upd，sctp，dccp。不同的协议处理流程大致是一样的，只是封装的报文以及头、尾部信息会有所不同
+ ip：以 IP 协议为例，它会将来自上一层（本例中的 TCP 报文）的数据包作为报文体，再次加入自己的报文头，譬如指明数据应该发到哪里的路由地址、数据包的长度、协议的版本号，等等，封装成 IP 数据包后发往下一层
+ device： Device 只是一种向操作系统端开放的接口，其背后既可能代表着真实的物理硬件，也可能是某段具有特定功能的程序代码，譬如即使不存在物理网卡，也依然可以存在回环设备（Loopback Device）
+ driver：网卡驱动程序（Driver）是网络访问层中面向硬件一侧的接口，网卡驱动程序会通过[DMA](https://en.wikipedia.org/wiki/Direct_memory_access)将主存中的待发送的数据包复制到驱动内部的缓冲区之中

#### 干预网络通信

从 Linux Kernel 2.4 版开始，内核开放了一套通用的、可供代码干预数据在协议栈中流转的过滤器框架--Netfilter。

Netfilter 框架围绕网络层（IP 协议）的周围，埋下了五个钩子，

每当有数据包流到网络层，经过这些钩子时，就会自动触发由内核模块注册在这里的回调函数，程序代码就能够通过回调来干预 Linux 的网络通信。

- PREROUTING：来自设备的数据包进入协议栈后立即触发此钩子。PREROUTING 钩子在进入 IP 路由之前触发。一般用于目标网络地址转换（Destination NAT，DNAT）。
- INPUT：报文经过 IP 路由后，如果确定是发往本机的，将会触发此钩子，一般用于加工发往本地进程的数据包。
- FORWARD：报文经过 IP 路由后，如果确定**不**是发往本机的，将会触发此钩子，一般用于处理转发到其他机器的数据包。
- OUTPUT：从本机程序发出的数据包，在经过 IP 路由前，将会触发此钩子，一般用于加工本地进程的输出数据包。
- POSTROUTING：从本机网卡出去的数据包，无论是本机的程序所发出的，还是由本机转发给其他机器的，都会触发此钩子，一般用于源网络地址转换（Source NAT，SNAT）。

<img src="/Users/wangfusheng/Documents/notes/分布式/凤凰架构/.assets/image-20220903134443469-2183884.png" alt="image-20220903134443469" style="zoom: 50%;" />  



#### 虚拟化网络设备

##### 虚拟网卡：tun/tap、veth

tun 和 tap 是两个相对独立的虚拟网络设备，其中 tap 模拟了以太网设备，操作二层数据包（以太帧），tun 则模拟了网络层设备，操作三层数据包（IP 报文）。

使用 tun/tap 设备的目的是实现把来自协议栈的数据包先交由某个打开了`/dev/net/tun`字符设备的用户进程处理后，再把数据包重新发回到链路中。

<img src="/Users/wangfusheng/Documents/notes/分布式/凤凰架构/.assets/image-20220903172700986-2197222.png" alt="image-20220903172700986" style="zoom:50%;" /> 

应用程序通过 tun 设备对外发送数据包后，另一端的字符设备已被 VPN 程序打开，便会把数据包通过字符设备发送给 VPN 程序，VPN 收到数据包，会修改后再重新封装成新报文。这种将一个数据包套进另一个数据包中的处理方式被形象地形容为“隧道”（Tunneling）

veth 是另一种主流的虚拟网卡方案，它应该相当于由交叉网线连接的**一对**物理网卡

<img src="/Users/wangfusheng/Documents/notes/分布式/凤凰架构/.assets/image-20220825143426917-1409268.png" alt="image-20220825143426917" style="zoom:33%;" /> 

##### 交换机：Linux Bridge

当有二层数据包（以太帧）从网卡进入 Linux Bridge，它将根据数据包的类型和目标 MAC 地址，按如下规则转发处理：

- 如果数据包是广播帧，转发给所有接入网桥的设备。
- 如果数据包是单播帧：
  - 且 MAC 地址在地址转发表中不存在，则[洪泛](https://en.wikipedia.org/wiki/Flooding_(computer_networking))（Flooding）给所有接入网桥的设备，并将响应设备的接口与 MAC 地址学习（MAC Learning）到自己的 MAC 地址转发表中。
  - 且 MAC 地址在地址转发表中已存在，则直接转发到地址表中指定的设备。
- 如果数据包是此前转发过的，又重新发回到此 Bridge，说明冗余链路产生了环路。对于这种数据包就需要交换机实现[生成树协议](https://en.wikipedia.org/wiki/Spanning_Tree_Protocol)（Spanning Tree Protocol，STP）来交换拓扑信息，生成唯一拓扑链路以切断环路

，Linux Bridge 允许给自己设置 IP 地址，比普通交换机多出一种特殊的转发情况：

- 如果数据包的目的 MAC 地址为网桥本身，并且网桥有设置了 IP 地址的话，那该数据包即被认为是收到发往创建网桥那台主机的数据包，此数据包将不会转发到任何设备，而是直接交给上层（三层）协议栈去处理

网桥就取代了 eth0 设备来对接协议栈，进行三层协议的处理。设置这条特殊转发规则的好处是：只要通过简单的 NAT 转换，就可以实现一个最原始的单 IP 容器网络



##### 网络：VXLAN

有了虚拟化网络设备后，下一步就是要使用这些设备组成网络，容器分布在不同的物理主机上，每一台物理主机都有物理网络相互联通，然而这种网络的物理拓扑结构是相对固定的，很难跟上云原生时代的分布式系统的逻辑拓扑结构变动频率，譬如服务的扩缩、断路、限流，等等，都可能要求网络跟随做出相应的变化。正因如此，软件定义网络（Software Defined Network，SDN）的需求在云计算和分布式时代变得前所未有地迫切，SDN 的核心思路是在物理的网络之上再构造一层虚拟化的网络，将控制平面和数据平面分离开来，实现流量的灵活控制，为核心网络及应用的创新提供良好的平台。SDN 里位于下层的物理网络被称为 Underlay，它着重解决网络的连通性与可管理性，位于上层的逻辑网络被称为 Overlay，它着重为应用提供与软件需求相符的传输服务和网络拓扑。

由于二层网络本身的工作特性决定了它非常依赖于广播，无论是广播帧（如 ARP 请求、DHCP、RIP 都会产生广播帧），还是泛洪路由，其执行成本都随着接入二层网络设备数量的增长而等比例增加，当设备太多，广播又频繁的时候，很容易就会形成[广播风暴](https://en.wikipedia.org/wiki/Broadcast_radiation)（Broadcast Radiation）。因此，VLAN 的首要职责就是划分广播域，将连接在同一个物理网络上的设备区分开来，划分的具体方法是在以太帧的报文头中加入 VLAN Tag，让所有广播只针对具有相同 VLAN Tag 的设备生效。这样既缩小了广播域，也附带提高了安全性和可管理性，因为两个 VLAN 之间不能直接通信



##### 副本网卡：MACVLAN

两个 VLAN 之间是完全二层隔离的，不存在重合的广播域，因此要通信就只能通过三层设备，最简单的三层通信就是靠单臂路由

<img src="/Users/wangfusheng/Documents/notes/分布式/凤凰架构/.assets/image-20220903200847044-2206928.png" alt="image-20220903200847044" style="zoom: 50%;" /> 

MACVLAN 允许对同一个网卡设置多个 IP 地址，也允许对同一张网卡上设置多个 MAC 地址，这也是 MACVLAN 名字的由来。原本 MAC 地址是网卡接口的“身份证”，应该是严格的一对一关系，而 MACVLAN 打破这层关系，方法是在物理设备之上、网络栈之下生成多个虚拟的 Device，每个 Device 都有一个 MAC 地址，新增 Device 的操作本质上相当于在系统内核中注册了一个收发特定数据包的回调函数，每个回调函数都能对一个 MAC 地址的数据包进行响应，当物理设备收到数据包时，会先根据 MAC 地址进行一次判断，确定交给哪个 Device 来处理，如图 12-8 所示。以交换机一侧的视角来看，这个端口后面仿佛是另一台已经连接了多个设备的交换机一样<img src="/Users/wangfusheng/Documents/notes/分布式/凤凰架构/.assets/image-20220903202416585-2207857.png" alt="image-20220903202416585" style="zoom:50%;" /> 

 

### 容器间的通信

##### docker三种网络方案

+ 桥接模式，使用`--network=bridge`指定。Docker 会为新容器分配独立的网络名称空间，创建好 veth pair，一端接入容器，另一端接入到 docker0 网桥上。Docker 为每个容器自动分配好 IP 地址，默认配置下地址范围是 172.17.0.0/24，docker0 的地址默认是 172.17.0.1，并且设置所有容器的网关均为 docker0，这样所有接入同一个网桥内的容器直接依靠二层网络来通信，在此范围之外的容器、主机就必须通过网关来访问
+ 主机模式，使用`--network=host`指定。主机模式下，Docker 不会为新容器创建独立的网络名称空间，这样容器一切的网络设施，如网卡、网络栈等都直接使用宿主机上的真实设施，容器也就不会拥有自己独立的 IP 地址。此模式下与外界通信无须进行 NAT 转换，没有性能损耗，但缺点也十分明显，没有隔离就无法避免网络资源的冲突，譬如端口号就不允许重复
+ **空置模式**，使用`--network=none`指定，空置模式下，Docker 会给新容器创建独立的网络名称空间，但是不会创建任何虚拟的网络设备，此时容器能看到的只有一个回环设备（Loopback Device）而已。提供这种方式是为了方便用户去做自定义的网络配置，如自己增加网络设备、自己管理 IP 地址

Docker 还支持以下由用户自行创建的网络：

+ **容器模式**，创建容器后使用`--network=container:容器名称`指定。容器模式下，新创建的容器将会加入指定的容器的网络名称空间，共享一切的网络资源，但其他资源，如文件、PID 等默认仍然是隔离的。两个容器间可以直接使用回环地址（localhost）通信，端口号等网络资源不能有冲突
+ **MACVLAN 模式**：使用`docker network create -d macvlan`创建，此网络允许为容器指定一个副本网卡，容器通过副本网卡的 MAC 地址来使用宿主机上的物理设备，在追求通信性能的场合，这种网络是最好的选择。Docker 的 MACVLAN 只支持 Bridge 通信模式，因此在功能表现上与桥接模式相类似
+ **Overlay 模式**：使用`docker network create -d overlay`创建，Docker 说的 Overlay 网络实际上就是特指 VXLAN，这种网络模式主要用于 Docker Swarm 服务之间进行通信。然而由于 Docker Swarm 败于 Kubernetes，并未成为主流，所以这种网络模式实际很少使用。

### 容器网络的生态

CNM 和 CNI 的网络插件提供的能力都能划分为网络的管理与 IP 地址的管理两类，插件可以选择只实现其中的某一个，也可以全部都实现：

+ **管理网络创建与删除**。
+ **管理 IP 地址分配与回收**。三层网络分配唯一的 IP 地址的问题，二层网络的 MAC 地址天然就具有唯一性，无须刻意地考虑如何分配的问题。

### 网络插件生态

+ overlay模式： 
+ **路由模式**：路由模式其实属于 Underlay 模式的一种特例
+ **Underlay 模式**：这里的 Underlay 模式特指让容器和宿主机处于同一网络，两者拥有相同的地位的网络方案。Underlay 网络要求容器的网络接口能够直接与底层网络进行通信，因此该模式是直接依赖于虚拟化设备与底层网络能力的



## 容器的持久化设计

用户负责以资源和声明式 API 来描述自己的意图，Kubernetes 负责根据用户意图来完成具体的操作。

#### Mount 和 Volume

Mount 是动词，表示将某个外部存储挂载到系统中，Volume 是名词，表示物理存储的逻辑抽象，目的是为物理存储提供有弹性的分割方式。容器源于对操作系统层面的虚拟化，为了满足容器内生成数据的外部存储需求，也自然可以把mount和volume延伸到容器中。

docker支持三种挂载：Bind（`--mount type=bind`）、Volume（`--mount type=volume`）和 tmpfs（`--mount type=tmpfs`

<img src="/Users/wangfusheng/Documents/notes/分布式/凤凰架构/.assets/image-20220826101341438-1480022.png" alt="image-20220826101341438" style="zoom: 50%;" />  

Bind Mount 只能让容器与本地宿主机之间建立了某个目录的映射，如果想要在不同宿主机上的容器共享同一份存储，就必须先把共享存储挂载到每一台宿主机操作系统的某个目录下，然后才能逐个挂载到容器内使用，这种跨宿主机共享存储的场景如图。

<img src="/Users/wangfusheng/Documents/notes/分布式/凤凰架构/.assets/image-20220826102844793-1480925.png" alt="image-20220826102844793" style="zoom: 50%;" />  

### 静态存储分配

Kubernetes 将 Volume 分为持久化的 PersistentVolume 和非持久化的普通 Volume 两类。

普通的volume的设计目标是为了同一个pod中多个容器可以共享存储资源，所以volume生命周期和挂载的它的pod相同。

<img src="/Users/wangfusheng/Documents/notes/分布式/凤凰架构/.assets/image-20220827105739435-1569060.png" alt="image-20220827105739435" style="zoom: 50%;" /> 

PersistentVolume 是由管理员负责提供的集群存储。
PersistentVolumeClaim 是由用户负责提供的存储请求。

<img src="/Users/wangfusheng/Documents/notes/分布式/凤凰架构/.assets/image-20220827110340451-1569421.png" alt="image-20220827110340451" style="zoom:50%;" />



### 动态存储分配

一旦应用规模增大，PersistentVolume 很难被自动化的问题就会突显出来。

动态存储分配方案，是指在用户声明存储能力的需求时，不是期望通过 Kubernetes 撮合来获得一个管理员人工预置的 PersistentVolume，而是由特定的资源分配器（Provisioner）自动地在存储资源池或者云存储系统中分配符合用户存储需要的 PersistentVolume，然后挂载到 Pod 中使用，完成这项工作的资源被命名为 StorageClass，它的具体工作过程如下：

1. 管理员根据存储系统的实际情况，先准备好对应的 Provisioner。Kubernetes 官方已经提供了一系列[预置的 In-Tree Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/)，放置在`kubernetes.io`的 API 组之下。其中部分 Provisioner 已经有了官方的 CSI 驱动，譬如 vSphere 的 Kubernetes 自带驱动为`kubernetes.io/vsphere-volume`，VMware 的官方驱动为`csi.vsphere.vmware.com`。
2. 管理员不再是手工去分配 PersistentVolume，而是根据存储去配置 StorageClass。Pod 是可以动态扩缩的，而存储则是相对固定的，哪怕使用的是具有扩展能力的云存储，也会将它们视为存储容量、IOPS 等参数可变的固定存储来看待，
3. 用户依然通过 PersistentVolumeClaim 来声明所需的存储，但是应在声明中明确指出该由哪个 StorageClass 来代替 Kubernetes 处理该 PersistentVolumeClaim 的请求
4. 如果 PersistentVolumeClaim 中要求的 StorageClass 及它用到的 Provisioner 均是可用的话，那这个 StorageClass 就会接管掉原本由 Kubernetes 撮合 PersistentVolume 与 PersistentVolumeClaim 的操作，按照 PersistentVolumeClaim 中声明的存储需求，自动产生出满足该需求的 PersistentVolume 描述信息，并发送给 Provisioner 处理
5. Provisioner 接收到 StorageClass 发来的创建 PersistentVolume 请求后，会操作其背后存储系统去分配空间，如果分配成功，就生成并返回符合要求的 PersistentVolume 给 Pod 使用。

<img src="/Users/wangfusheng/Documents/notes/分布式/凤凰架构/.assets/image-20220827112526584-1570727.png" alt="image-20220827112526584" style="zoom:33%;" />



## 容器存储与生态

Kubernetes 参考了传统操作系统接入或移除新存储设备做法，把接入或移除外部存储这件事情分解为以下三种操作：

+ 首先，决定应**准备**（Provision）何种存储，Provision 可类比为给操作系统扩容而购买了新的存储设备。这步确定了接入存储的来源、容量、性能以及其他技术参数，它的逆操作是**移除**（Delete）存储
+ 然后，将准备好的存储**附加**（Attach）到系统中，Attach 可类比为将存储设备接入操作系统，此时尽管设备还不能使用。这步确定了存储的设备名称、驱动方式等面向系统一侧的信息，它的逆操作是**分离**（Detach）存储设备。
+ 最后，将附加好的存储**挂载**（Mount）到系统中，Mount 可类比为将设备挂载到系统的指定位置，也就是操作系统中`mount`命令的作用。这步确定了存储的访问目录、文件系统格式等面向应用一侧的信息，它的逆操作是**卸载**（Unmount）存储设备。

Provision、Delete、Attach、Detach、Mount、Unmount 六种操作

+ **PV 控制器**（PersistentVolume Controller）：“[以容器构建系统](https://icyfenix.cn/immutable-infrastructure/container/container-build-system.html#韧性与弹性)”一节中介绍过，Kubernetes 里所有的控制器都遵循着相同的工作模式——让实际状态尽可能接近期望状态
+ **AD 控制器**（Attach/Detach Controller）：AD 控制器的期望状态是“所有被调度到准备新创建 Pod 的节点，都附加好了要使用的存储；当 Pod 被销毁后，原本运行 Pod 的节点都分离了不再被使用的存储”，如果实际状态不符合该期望，会根据需要调用存储驱动插件的 Attach/Detach 操作。
+ **Volume 管理器**（Volume Manager）：Volume 管理器实际上是 kubelet 的一部分，是 kubelet 中众多管理器的其中一个，它主要是用来支持本节点中 Volume 执行 Attach/Detach/Mount/Unmount 操作。你可能注意到这里不仅有 Mount/Unmount 操作，也出现了 Attach/Detach 操作，这是历史原因导致的，由于最初版本的 Kubernetes 中并没有 AD 控制器，Attach/Detach 的职责也在 kubelet 中完成。现在 kubelet 默认情况下已经不再会执行 Attach/Detach 了，但有少量旧程序已经依赖了由 kubelet 来实现 Attach/Detach 的内部逻辑，所以 kubelet 不得不设计一个`--enable-controller-attach-detach`参数，如果将其设置为`false`的话就会重新回到旧的兼容模式上，由 kubelet 代替 AD 控制器来完成 Attach/Detach。

<img src="/Users/wangfusheng/Documents/notes/分布式/凤凰架构/.assets/image-20220828002736400-1617659.png" alt="image-20220828002736400" style="zoom:50%;" /> 

### flexVolume 与CSI: 存储扩展机制

FlexVolume 驱动其实就是一个实现了 Attach、Detach、Mount、Unmount 操作的可执行文件。



### 容器插件生态

存储系统和设备均可以划分到块存储、文件存储和对象存储这三种存储类型之中，划分的根本依据其实并非各种存储是如何储存数据的。



### 资源与调度

#### 资源模型

Kubernetes 系统中所有你能够接触的方方面面都被抽象成了资源。
“一切皆为资源”的设计是 Kubernetes 能够顺利施行声明式 API 的必要前提，Kubernetes 以资源为载体，建立了一套同时囊括了抽象元素（如策略、依赖、权限）和物理元素（如软件、硬件、网络）的领域特定语言。
通过不同层级间资源的使用关系来描述上至整个集群甚至是集群联邦，下至某一块内存区域或者一小部分的处理器核心的状态，这些对资源状态的描述的集合，共同构成了一幅信息系统工作运行的全景图。

从编排系统的角度来看，Node 是资源的提供者，Pod 是资源的使用者，调度是将两者进行恰当的撮合。Node 通常能够提供的三方面的资源：计算资源（如处理器、图形处理器、内存）、存储资源（如磁盘容量、不同类型的介质）和网络资源（如带宽、网络地址）其中与调度关系最密切的是处理器和内存，虽然它们同属于计算资源，但两者在调度时又有一些微妙的差别：处理器这样的资源被称作可压缩资源（Compressible Resources），特点是当可压缩资源不足时，Pod 只会处于“饥饿状态”，运行变慢，但不会被系统杀死，即容器被直接终止，或被要求限时退出。而像内存这样的资源，则被称作不可压缩资源（Incompressible Resources），特点是当不可压缩资源不足，或者超过了容器自己声明的最大限度时，Pod 就会因为内存溢出（Out-Of-Memory，OOM）而被系统直接杀掉。

### 服务质量和优先级

设定资源计量单位的目的是为了管理员能够限制某个 Pod 对资源的过度占用，避免影响到其他 Pod 的正常运行。Pod 是由一到多个容器所组成，资源最终是交由 Pod 的各个容器去使用，所以资源的需求是设定在容器上的，具体的配置是 Pod 的`spec.containers[].resource.limits/requests.cpu/memory`字段

kubernetes 目前提供的服务质量等级一共分为三级，由高到低分别为 Guaranteed、Burstable 和 BestEffort。

limits = requests, Guaranteed,
limits > request(或者没设置)， Burstable
limits 和 request 都没有设置， BestEffort

### 驱逐机制

##### 不可压缩资源：

可用内存（`memory.available`）

宿主机的可用磁盘空间（`nodefs.available`）

文件系统可用[inode](https://en.wikipedia.org/wiki/Inode)数量（`nodefs.inodesFree`）

可用的容器运行时镜像存储空间（`imagefs.available`）

```
memory.available < 100Mi
nodefs.available < 10%
nodefs.inodesFree < 5%
imagefs.available < 15%
```

##### 驱逐机制

**软驱逐**（Soft Eviction）、**硬驱逐**（Hard Eviction）以及**优雅退出期**（Grace Period）：

- **软驱逐**：通常配置一个较低的警戒线（譬如可用内存仅剩 20%），触及此线时，系统将进入一段观察期。如果只是暂时的资源抖动，在观察期内能够恢复到正常水平的话，那就不会真正启动驱逐操作。否则，资源持续超过警戒线一段时间，就会触发 Pod 的优雅退出（Grace Shutdown），系统会通知 Pod 进行必要的清理工作（譬如将缓存的数据落盘），然后自行结束。在优雅退出期结束后，系统会强制杀掉还未曾自行了断的 Pod。
- **硬驱逐**：通常配置一个较高的终止线（譬如可用内存仅剩 10%），一旦触及此红线，立即强制杀掉 Pod，不理会优雅退出。

### 默认调度器







### 服务网格

服务网格是一种用于管控服务间通信的的基础设施，职责是为现代云原生应用支持网络请求在复杂的拓扑环境中可靠地传递。在实践中，服务网格通常会以轻量化网络代理的形式来体现，这些代理与应用程序代码会部署在一起，对应用程序来说，它完全不会感知到代理的存在。

#### 通信的成本

**第一阶段：将通信的非功能性需求视作业务需求的一部分，通信的可靠性由程序员来保障**。

<img src="/Users/wangfusheng/Documents/notes/分布式/凤凰架构/.assets/image-20220829150446972-1756688.png" alt="image-20220829150446972" style="zoom:67%;" />

**第二阶段：将代码中的通信功能抽离重构成公共组件库，通信的可靠性由专业的平台程序员来保障**。

**第三阶段：将负责通信的公共组件库分离到进程之外，程序间通过网络代理来交互，通信的可靠性由专门的网络代理提供商来保障**。

<img src="/Users/wangfusheng/Documents/notes/分布式/凤凰架构/.assets/image-20220829150845462-1756926.png" alt="image-20220829150845462" style="zoom:50%;" /> 

**第四阶段：将网络代理以边车的形式注入到应用容器，自动劫持应用的网络流量，通信的可靠性由专门的通信基础设施来保障**。



#### 数据平面

